{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from random import randint\n",
    "from sklearn.decomposition import PCA\n",
    "#download PCA\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "import json\n",
    "from catboost import CatBoostRegressors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import re\n",
    "import lightgbm as lgb\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepwalk on matrix structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exact code from the last seminar\n",
    "def random_walk(G, node, walk_length):\n",
    "    # Simulates a random walk of length \"walk_length\" starting from node \"node\"\n",
    "\n",
    "    #Please insert your code for Task 1 here\n",
    "    walk = [node]\n",
    "    for i in range(walk_length):\n",
    "        neighbors =  list(G.neighbors(walk[i]))\n",
    "        walk.append(neighbors[randint(0,len(neighbors)-1)])\n",
    "\n",
    "    walk = [str(node) for node in walk]\n",
    "    return walk\n",
    "\n",
    "def generate_walks(G, num_walks, walk_length):\n",
    "    # Runs \"num_walks\" random walks from each node\n",
    "\n",
    "    walks = []\n",
    "\n",
    "    #Please insert your code for Task 1 here\n",
    "    for i in range(num_walks):\n",
    "        permuted_nodes = np.random.permutation(G.nodes())\n",
    "        for node in permuted_nodes:\n",
    "            walks.append(random_walk(G, node, walk_length))\n",
    "\n",
    "    return walks\n",
    "\n",
    "def deepwalk(G, num_walks, walk_length, n_dim):\n",
    "    # Simulates walks and uses the Skipgram model to learn node representations\n",
    "\n",
    "    print(\"Generating walks\")\n",
    "    walks = generate_walks(G, num_walks, walk_length)\n",
    "\n",
    "    print(\"Training word2vec\")\n",
    "    \n",
    "    #Please insert your code for Task 2 here\n",
    "    model = Word2Vec(vector_size=n_dim, window=8, min_count=0, sg=1, workers=8, hs=1)\n",
    "    \n",
    "    \n",
    "    model.build_vocab(walks)\n",
    "    model.train(walks, total_examples=model.corpus_count, epochs=5)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 217801\n",
      "Number of edges: 1718164\n"
     ]
    }
   ],
   "source": [
    "# read training data\n",
    "df_train = pd.read_csv('train.csv', dtype={'author': np.int64, 'hindex': np.float32})\n",
    "n_train = df_train.shape[0]\n",
    "\n",
    "# read test data\n",
    "df_test = pd.read_csv('test.csv', dtype={'author': np.int64})\n",
    "n_test = df_test.shape[0]\n",
    "\n",
    "# load the graph    \n",
    "G = nx.read_edgelist('coauthorship.edgelist', delimiter=' ', nodetype=int)\n",
    "n_nodes = G.number_of_nodes()\n",
    "n_edges = G.number_of_edges() \n",
    "print('Number of nodes:', n_nodes)\n",
    "print('Number of edges:', n_edges)\n",
    "core_number = nx.core_number(G)\n",
    "pr = nx.pagerank(G)\n",
    "for el in pr:\n",
    "  pr[el] = pr[el]*1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "n_dim = 50\n",
    "n_walks = 5\n",
    "walk_length = 8\n",
    "model = deepwalk(G, n_walks, walk_length, n_dim) \n",
    "print(time.time()-start, \"time for deep walk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating on n_dim = 50, n_walks = 5, walk_length = 8  413 seconds\n",
    "# Calculating on n_dim = 64, n_walks = 5, walk_length = 14  585 seconds\n",
    "# Calculating on n_dim = 80, n_walks = 7, walk_length = 20  585 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepWalk_embeddings = np.empty(shape=(df_train.shape[0]+df_test.shape[0], n_dim))\n",
    "for idx,node in enumerate(G.nodes()):\n",
    "    DeepWalk_embeddings[idx,:] = model.wv[str(node)]\n",
    "\n",
    "nodes_graph = list(G.nodes())\n",
    "vec_embed = {} # nodes corresponds to deepwalks_embeddings\n",
    "for el in range(len(nodes_graph)):\n",
    "    vec_embed[nodes_graph[el]] = DeepWalk_embeddings[el] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((n_train, 5))\n",
    "y_train = np.zeros(n_train)\n",
    "for i,row in df_train.iterrows():\n",
    "    node = row['author']\n",
    "    X_train[i, 0] = node\n",
    "    X_train[i,1] = G.degree(node)\n",
    "    X_train[i,2] = core_number[node]\n",
    "    X_train[i,3] = pr[node]\n",
    "    y_train[i] = row['hindex']\n",
    "\n",
    "# create test matrix\n",
    "X_test = np.zeros((n_test, 5))\n",
    "for i,row in df_test.iterrows():\n",
    "    node = row['author']\n",
    "    X_test[i, 0] = node\n",
    "    X_test[i,1] = G.degree(node)\n",
    "    X_test[i,2] = core_number[node]\n",
    "    X_test[i,3] = pr[node]\n",
    "    \n",
    "X_train_test = np.concatenate((X_train, X_test), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_vec_embed = []\n",
    "for i,row in df_train.iterrows():\n",
    "    node = row['author']\n",
    "    X_train[i, 0] = node\n",
    "    arr_vec_embed.append(vec_embed[node])\n",
    "for i,row in df_test.iterrows():\n",
    "    node = row['author']\n",
    "    X_test[i, 0] = node\n",
    "    arr_vec_embed.append(vec_embed[node])\n",
    "    \n",
    "    \n",
    "df_arr= pd.DataFrame(data=arr_vec_embed) #dataframe of deepwalk embeddings\n",
    "\n",
    "vec_embed = {}\n",
    "arr_vec_embed = []\n",
    "pr = []\n",
    "model = 0\n",
    "DeepWalk_embeddings = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_arr.to_pickle('deep_walk50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4063264559353831, 0.0)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats.stats import pearsonr   \n",
    "  \n",
    "pearsonr(X_train[:, 1],y_train) # correlation of degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3590667720340064, 0.0)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(X_train[:, 2],y_train) # corrrelation of core_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.40216429459635095, 0.0)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(X_train[:, 3],y_train) #pagerank correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3631428505021522, 0.0)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(np.squeeze(mass_1[0:y_train.shape[0]]),y_train) # for average h_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "119.17308807373047\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "with open('abstracts.txt') as f:\n",
    "    lines1 = f.readlines()\n",
    "arr2 = []\n",
    "indeces = []\n",
    "for elem in range(0, len(lines1)):\n",
    "    if elem % 100000 == 0:\n",
    "        print(elem)\n",
    "    mass = lines1[elem]\n",
    "    indeces.append(int(mass.split('----')[0]))\n",
    "    j = mass.split('----')[1]\n",
    "    try:\n",
    "        d = json.loads(j)\n",
    "        mass1 = d['InvertedIndex']\n",
    "        dl = d['IndexLength']\n",
    "        arr1 = np.array(['ookdfnjdfmkdf' for _ in range(dl)])\n",
    "        for el in mass1:\n",
    "            for el1 in mass1[el]:\n",
    "                arr1[el1] = el\n",
    "    #arr2.append(arr1)\n",
    "        arr2.append(' '.join(arr1))\n",
    "    except ValueError:\n",
    "        arr2.append(' ')\n",
    "print(time.time()-start)\n",
    "\n",
    "start = time.time()\n",
    "abstracts = pd.DataFrame(index = indeces, data =arr2)\n",
    "abstracts = abstracts.reset_index(level = [0])\n",
    "abstracts = abstracts.set_axis(['paper_id', 'article'], axis='columns', inplace=False)\n",
    "#abstracts.to_pickle('abstracts_table') # to save file\n",
    "#abstracts = pd.read_pickle('abstracts_table') # to read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for el in range(len(indeces)):\n",
    "    d[indeces[el]] = arr2[el]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('author_papers.txt') as f:\n",
    "    lines1 = f.readlines()\n",
    "    \n",
    "indic = []\n",
    "ids = []\n",
    "d_num_papers = {}\n",
    "arr = []\n",
    "for i in range(0,len(lines1) ):\n",
    "    cur_line = lines1[i].split(':')\n",
    "    indic.append(int(cur_line[0]))\n",
    "    d_num_papers[int(cur_line[0])] = len(cur_line[1].split('\\n')[0].split('-'))\n",
    "    arr.append(len(cur_line[1].split('\\n')[0].split('-')))\n",
    "    ids.append(cur_line[1].split('\\n')[0].split('-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "d1 = {'id':[1036332], 'article1': [1510273386], 'article2': [1827736641], 'article3':[1588673897], 'article4':[2252711322], 'article5':[2123653597]}\n",
    "df_articles = pd.DataFrame(data=d1)\n",
    "\n",
    "for i in range(0, len(ids)):\n",
    "    cur_arr = np.zeros(6)\n",
    "    cur_arr[0] = indic[i] \n",
    "    for j in range(len(ids[i])):\n",
    "        #new_row = {'id':indic[i], 'articles':ids[i][j]} \n",
    "        cur_arr[j+1] = ids[i][j]\n",
    "        \n",
    "        \n",
    "        #df_articles = df_articles.append(new_row, ignore_index=True)\n",
    "        df_articles.loc[i] = cur_arr\n",
    "    if i % 50000 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>articles1</th>\n",
       "      <th>articles2</th>\n",
       "      <th>articles3</th>\n",
       "      <th>articles4</th>\n",
       "      <th>articles5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1036332.0</td>\n",
       "      <td>1.510273e+09</td>\n",
       "      <td>1.827737e+09</td>\n",
       "      <td>1.588674e+09</td>\n",
       "      <td>2.252711e+09</td>\n",
       "      <td>2.123654e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101850.0</td>\n",
       "      <td>1.334590e+08</td>\n",
       "      <td>1.797197e+08</td>\n",
       "      <td>2.111788e+09</td>\n",
       "      <td>2.126489e+09</td>\n",
       "      <td>3.183900e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1336878.0</td>\n",
       "      <td>2.122092e+09</td>\n",
       "      <td>2.132110e+09</td>\n",
       "      <td>2.100272e+09</td>\n",
       "      <td>2.065673e+09</td>\n",
       "      <td>2.036414e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1515524.0</td>\n",
       "      <td>2.141828e+09</td>\n",
       "      <td>2.127086e+09</td>\n",
       "      <td>2.013548e+09</td>\n",
       "      <td>2.138530e+09</td>\n",
       "      <td>1.994864e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1606427.0</td>\n",
       "      <td>1.907725e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     articles1     articles2     articles3     articles4  \\\n",
       "0  1036332.0  1.510273e+09  1.827737e+09  1.588674e+09  2.252711e+09   \n",
       "2  1101850.0  1.334590e+08  1.797197e+08  2.111788e+09  2.126489e+09   \n",
       "3  1336878.0  2.122092e+09  2.132110e+09  2.100272e+09  2.065673e+09   \n",
       "4  1515524.0  2.141828e+09  2.127086e+09  2.013548e+09  2.138530e+09   \n",
       "5  1606427.0  1.907725e+09  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "\n",
       "      articles5  \n",
       "0  2.123654e+09  \n",
       "2  3.183900e+07  \n",
       "3  2.036414e+09  \n",
       "4  1.994864e+09  \n",
       "5  0.000000e+00  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_articles = pd.read_pickle('articles_table')\n",
    "df_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abstracts.to_pickle('articles_table') # to save file\n",
    "#df_articles = pd.read_pickle('articles_table') # to read from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_paper_first = df_articles[['articles1']].to_numpy()\n",
    "author_papers_id = df_articles[['id']].to_numpy()\n",
    "id_paper_second = df_articles[['articles2']].to_numpy()\n",
    "id_paper_third = df_articles[['articles3']].to_numpy()\n",
    "id_paper_four = df_articles[['articles4']].to_numpy()\n",
    "id_paper_five = df_articles[['articles5']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_author_id_article1= {}\n",
    "d_author_id_article2= {}\n",
    "d_author_id_article3= {}\n",
    "d_author_id_article4= {}\n",
    "d_author_id_article5= {}\n",
    "for i in range(len(id_paper_first)):\n",
    "    d_author_id_article1[author_papers_id[i][0]] = id_paper_first[i][0]\n",
    "    d_author_id_article2[author_papers_id[i][0]] = id_paper_second[i][0]\n",
    "    d_author_id_article3[author_papers_id[i][0]] = id_paper_third[i][0]\n",
    "    d_author_id_article4[author_papers_id[i][0]] = id_paper_four[i][0]\n",
    "    d_author_id_article5[author_papers_id[i][0]] = id_paper_five[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20000\n",
      "40000\n",
      "60000\n",
      "80000\n",
      "100000\n",
      "120000\n",
      "140000\n",
      "160000\n",
      "180000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "first_article = []\n",
    "second_article = []\n",
    "third_article = []\n",
    "four_article = []\n",
    "five_article = []\n",
    "fl = 0\n",
    "for i in range(X_train_test.shape[0]):\n",
    "    try:\n",
    "        el1 = d[d_author_id_article1[X_train_test[i, 0]]]\n",
    "        el2 = d[d_author_id_article2[X_train_test[i, 0]]]\n",
    "        el3 = d[d_author_id_article3[X_train_test[i, 0]]]\n",
    "        el4 = d[d_author_id_article4[X_train_test[i, 0]]]\n",
    "        el5 = d[d_author_id_article5[X_train_test[i, 0]]]\n",
    "        first_article.append(el1)\n",
    "        second_article.append(el2)\n",
    "        third_article.append(el3)\n",
    "        four_article.append(el4)\n",
    "        five_article.append(el5)\n",
    "        \n",
    "    except KeyError:\n",
    "        first_article.append(' ')\n",
    "        second_article.append(' ')\n",
    "        third_article.append(' ')\n",
    "        four_article.append(' ')\n",
    "        five_article.append(' ')\n",
    "        fl += 1\n",
    "    if i % 20000 == 0:\n",
    "        print(i)\n",
    "        \n",
    "# erase data, because dict use much memory\n",
    "d_author_id_article1= {}\n",
    "d_author_id_article2= {}\n",
    "d_author_id_article3= {}\n",
    "d_author_id_article4= {}\n",
    "d_author_id_article5= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train_main  = {'id':X_train[:, 0], 'article_1': first_article[0:174241], 'article_2': second_article[0:174241], 'article_3':third_article[0:174241], \n",
    "                 'article_4':four_article[0:174241], 'article_5':five_article[0:174241]}\n",
    "df_train_main = pd.DataFrame(data=d_train_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test_main = {'id':X_test[:, 0], 'article_1': first_article[174241:], 'article_2': second_article[174241:], 'article_3':third_article[174241:], \n",
    "                 'article_4':four_article[174241:], 'article_5':five_article[174241:]}\n",
    "df_test_main = pd.DataFrame(data=d_test_main)\n",
    "\n",
    "\n",
    "#df_train_main.to_pickle('train_article') # save to the file\n",
    "#df_test_main.to_pickle('test_article')\n",
    "\n",
    "#df_train_main = pd.read_pickle('train_article') #read from the file\n",
    "#df_test_main = pd.read_pickle('test_article')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making features using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"(@\\[A-Za-z]+)|([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", sentence)\n",
    "    #regular expression which deletes numbers \n",
    "    \n",
    "    #return \" \".join([word for word in sentence.split() if word not in (stop_words)])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train_main['articles_tog'] = '' #sum all articles together\n",
    "cur_mass = []\n",
    "for i in range(df_train_main.shape[0]):\n",
    "    cur_mass.append(df_train_main.loc[i].article_1 + df_train_main.loc[i].article_2+ df_train_main.loc[i].article_3+df_train_main.loc[i].article_4+df_train_main.loc[i].article_5)\n",
    "    if i % 50000 == 0:\n",
    "        print(i, \"Train\")\n",
    "\n",
    "df_train_main['articles_tog'] = cur_mass\n",
    "#df_train_main.to_pickle('train_article') # save results to df_train_main\n",
    "\n",
    "\n",
    "df_test_main['articles_tog'] = '' #same for test\n",
    "cur_mass = []\n",
    "for i in range(df_test_main.shape[0]):\n",
    "    cur_mass.append(df_test_main.loc[i].article_1 + df_test_main.loc[i].article_2+ df_test_main.loc[i].article_3+df_test_main.loc[i].article_4+df_test_main.loc[i].article_5)\n",
    "    if i % 50000 == 0:\n",
    "        print(i, \"Test\")\n",
    "\n",
    "df_test_main['articles_tog'] = cur_mass\n",
    "#df_test_main.to_pickle('test_article') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles_tog -> articles with summed all articles\n",
    "start = time.time()\n",
    "testing_clean = []\n",
    "\n",
    "for i in range(df_train_main.shape[0]):\n",
    "    sentence = clean_sentence(df_train_main.iloc[i].articles_tog)\n",
    "    sentence = \" \".join([word for word in sentence.split() if word not in (stop_words)])\n",
    "    testing_clean.append(sentence)\n",
    "    \n",
    "for i in range(df_test_main.shape[0]):\n",
    "    sentence = clean_sentence(df_test_main.iloc[i].articles_tog)\n",
    "    sentence = \" \".join([word for word in sentence.split() if word not in (stop_words)])\n",
    "    testing_clean.append(sentence)\n",
    "    \n",
    "print(time.time()-start, \"time for cleaning\")\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "tokenized_corpus = [wpt.tokenize(document) for document in testing_clean]\n",
    "\n",
    "start = time.time()\n",
    "# Set values for various parameters\n",
    "feature_size = 100      # Word vector dimensionality  \n",
    "window_context = 30     # Context window size                                                                                    \n",
    "min_word_count = 1      # Minimum word count                        \n",
    "sample = 1e-3           # Downsample setting for frequent words\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, \n",
    "                          window=window_context, min_count=min_word_count,\n",
    "                          sample=sample, iter=50)\n",
    "print(time.time()-start, \"Time for word2vec model\")\n",
    "\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "   \n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# get document level embeddings\n",
    "start = time.time()\n",
    "w2v_feature_array = averaged_word_vectorizer(corpus=tokenized_corpus, model=w2v_model,\n",
    "                                             num_features=feature_size)\n",
    "w2v_feature_array_1 = pd.DataFrame(w2v_feature_array) # Save data for word2vec features \n",
    "w2v_feature_array_1.to_pickle('w2v_feature_array_1')\n",
    "\n",
    "# features_word2vec = pd.read_pickle('w2v_feature_array_1') #upload file \n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating new features\n",
    "\n",
    "* 1 if author has 5 pages\n",
    "* average h_index of neigbours for each author\n",
    "* max_h_index of neighbours\n",
    "* min h_index of neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('author_papers.txt') as f:\n",
    "    lines1 = f.readlines()\n",
    "    \n",
    "d_num_papers = {}\n",
    "for i in range(0,len(lines1) ):\n",
    "    cur_line = lines1[i].split(':')\n",
    "    d_num_papers[int(cur_line[0])] = len(cur_line[1].split('\\n')[0].split('-'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_5 = [] # feature 1 if author has 5 pages and 0 otherwise\n",
    "for i in range(X_train_test.shape[0]):\n",
    "    el = d_num_papers[X_train_test[i][0]]\n",
    "    if el == 5:\n",
    "        new_features_5.append(1)\n",
    "    else:\n",
    "        new_features_5.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3082919120788574 average h_index for neighbours\n"
     ]
    }
   ],
   "source": [
    "d = {} # store for author -> h_index\n",
    "for i in range(X_train.shape[0]):\n",
    "    d[X_train[i][0]] = y_train[i]\n",
    "\n",
    "mass_1 = [] # feature for calculating average h_index\n",
    "start = time.time()\n",
    "for i in range(X_train_test.shape[0]):\n",
    "    \n",
    "    neighb = list(G.neighbors(X_train_test[i][0]))\n",
    "    ch = 0\n",
    "    col = 0\n",
    "    for neigh in neighb:\n",
    "    \n",
    "        if (neigh in d) == False:\n",
    "            ch += 0\n",
    "        else:\n",
    "            ch += float(d[neigh])\n",
    "            col += 1\n",
    "    if col > 0:\n",
    "        mass_1.append(ch/col)\n",
    "    else:\n",
    "        mass_1.append(0)\n",
    "print(time.time()-start, \"average h_index for neighbours\")\n",
    "\n",
    "mass_1 = np.array(mass_1)\n",
    "mass_1 = mass_1.reshape(-1, 1)\n",
    "new_features_5 = np.array(new_features_5)\n",
    "new_features_5 = new_features_5.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.661071062088013\n"
     ]
    }
   ],
   "source": [
    "mass_maxi = []\n",
    "mass_mini = []\n",
    "start = time.time()\n",
    "for i in range(X_train_test.shape[0]): \n",
    "    \n",
    "    neighb = list(G.neighbors(X_train_test[i][0]))\n",
    "    maxi = 0\n",
    "    mini = 400\n",
    "    for neigh in neighb:\n",
    "        \n",
    "        \n",
    "        if (neigh in d) == False:\n",
    "            maxi += 0\n",
    "            mini += 0\n",
    "        else:\n",
    "            if  float(d[neigh]) > maxi:\n",
    "                maxi = float(d[neigh])\n",
    "        \n",
    "            if  float(d[neigh]) < mini:\n",
    "                mini = float(d[neigh])\n",
    "                \n",
    "            \n",
    "    if mini == 400:\n",
    "        mass_mini.append(0)\n",
    "    else:\n",
    "        mass_mini.append(mini)\n",
    "    mass_maxi.append(maxi)\n",
    "\n",
    "print(time.time()-start)\n",
    "\n",
    "mass_maxi = np.array(mass_maxi)\n",
    "mass_maxi = mass_maxi.reshape(-1, 1)\n",
    "mass_mini = np.array(mass_mini)\n",
    "mass_mini = mass_mini.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_word2vec = pd.read_pickle('w2v_feature_array_1') # file of word2vec features\n",
    "df_arr = pd.read_pickle('deep_walk')\n",
    "'''abstracts = pd.read_pickle('abstracts_table')\n",
    "df_articles = pd.read_pickle('articles_table')\n",
    "tfidf_vector = pd.read_pickle('author_tfidf_vector.txt')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = np.hstack((X_train[:, 1:4], mass_1[0:174241],  new_features_5[0:174241], features_word2vec[0:174241], df_arr[0:174241]))\n",
    "final_test_test = np.hstack((X_test[:, 1:4],mass_1[174241:], new_features_5[174241:], features_word2vec[174241:],  df_arr[174241:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1, y_train1, y_test1 = train_test_split(final_test, y_train, test_size=.33, random_state = 42)\n",
    "start = time.time()\n",
    "clf = CatBoostRegressor(n_estimators = 1000,learning_rate = 0.2,  depth = 5)\n",
    "clf.fit(train1, y_train1)\n",
    "y_pred = clf.predict(test1)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Analyze for predictions and their distribution compared to real model\n",
    " 10 < x  11 to 15.5 37743 to 38812 \n",
    "10 < x < 20 77 to 39     12711 to 8963\n",
    "20 < x < 30 170 to 86.66  4127 to 3391 observations\n",
    "30  < x < 50  277 to 210 mse 2137 to 2484\n",
    "50 < x < 80 276 to 489 752 to 1049\n",
    "> 80 30 to 160 500 vs 2800 '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking quality of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estim = [100, 200, 300]\n",
    "learning = [0.1,0.12]\n",
    "num_leave = [20, 50, 100, 200]\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1121218)\n",
    "for el in n_estim:\n",
    "    cv_scores = np.empty(5)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(final_test, y_train)):\n",
    "        X_train, X_test = final_test[train_idx], final_test[test_idx]\n",
    "        y_train1, y_test1 = y_train[train_idx], y_train[test_idx]\n",
    "\n",
    "        model = lgb.LGBMRegressor(n_estimators = 500, learning_rate = 0.12, num_leaves = 200)\n",
    "        model.fit(X_train, y_train1)\n",
    "        preds = model.predict(X_test)\n",
    "        cv_scores[idx] = mean_squared_error(y_test1, preds)\n",
    "    print(cv_scores,np.mean(cv_scores),  el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_estimators\n",
    "'''[53.53571033 54.50651916 53.60322133] 100\n",
    "[51.77734503 52.61463367 51.70348592] 200\n",
    "[50.42872934 50.96430724 50.22325336] 500'''\n",
    "\n",
    "#lr\n",
    "#[51.38879843 52.64495793 51.90799202 51.37096127 50.84715438] 51.63197280673071 0.12\n",
    "#num_leaves\n",
    "\n",
    "#[52.96982495 53.52256497 52.76881649 52.04011541 51.48920142] 52.558104647817274 20\n",
    "#[51.3099745  51.78890643 50.82470602 50.65195368 50.0970291 ] 50.93451394471755 50\n",
    "#[50.16356784 51.15144598 50.80201971 49.90393926 49.69891136] 50.343976829826204 100\n",
    "#[50.06984358 51.40634118 50.22492043 50.51083878 49.07635478] 50.25765974824528 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use there final model with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "reg = lgb.LGBMRegressor(n_estimators = 700,learning_rate = 0.12, num_leaves = 150)\n",
    "reg.fit(final_test, y_train)\n",
    "y_pred = reg.predict(final_test_test)\n",
    "\n",
    "clf = CatBoostRegressor(n_estimators = 1000,learning_rate = 0.2,  depth = 7)\n",
    "clf.fit(final_test, y_train)\n",
    "y_pred1 = clf.predict(final_test_test)\n",
    "\n",
    "\n",
    "clf1 = CatBoostRegressor(n_estimators = 1000,learning_rate = 0.2,  depth = 7, random_state = 54)\n",
    "clf1.fit(final_test, y_train)\n",
    "y_pred2 = clf1.predict(final_test_test)\n",
    "print(time.time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1[y_pred1 <0] = 2 # 2 is better by checking local score on mse\n",
    "y_pred2[y_pred2 <0] = 2\n",
    "y_pred[y_pred <0] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = (y_pred2+y_pred1+y_pred)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving results to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to kaggle\n",
    "df_test['hindex'] = pd.Series(np.round_(final_results, decimals=3))\n",
    "df_test.loc[:,[\"author\",\"hindex\"]].to_csv('submission_second.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What we tried, but didn't work\n",
    "\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features = 200)\n",
    "X_train_tfidf = tfidf.fit_transform(df_test_data['article'].to_numpy())\n",
    "tdifd_arr = X_train_tfidf.toarray()\n",
    "\n",
    "X_train_tfidf2 = tfidf.fit_transform(df_train_main['article_2'].to_numpy())\n",
    "X_train_tfidf3 = tfidf.fit_transform(df_train_main['article_3'].to_numpy())\n",
    "X_train_tfidf4 = tfidf.fit_transform(df_train_main['article_4'].to_numpy())\n",
    "X_train_tfidf5 = tfidf.fit_transform(df_train_main['article_5'].to_numpy())\n",
    "\n",
    "tdifd_arr = X_train_tfidf.toarray()\n",
    "tdifd_arr2 = X_train_tfidf2.toarray()\n",
    "tdifd_arr3 = X_train_tfidf3.toarray()\n",
    "tdifd_arr4 = X_train_tfidf4.toarray()\n",
    "tdifd_arr5 = X_train_tfidf5.toarray()\n",
    "\n",
    "final_article1 = np.hstack((df_train_main[['degree', 'core']].to_numpy(), tdifd_arr,tdifd_arr2, tdifd_arr3, tdifd_arr4, tdifd_arr5 ))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
